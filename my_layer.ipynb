{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pywt\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((520, 4, 4, 120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Conv2d(120, 120* 3, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DWT_Function(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, w_ll, w_lh, w_hl, w_hh):\n",
    "        x = x.contiguous()\n",
    "        ctx.save_for_backward(w_ll, w_lh, w_hl, w_hh)\n",
    "        ctx.shape = x.shape\n",
    "\n",
    "        dim = x.shape[1]\n",
    "        x_ll = torch.nn.functional.conv2d(x, w_ll.expand(dim, -1, -1, -1), stride = 2, groups = dim)\n",
    "        x_lh = torch.nn.functional.conv2d(x, w_lh.expand(dim, -1, -1, -1), stride = 2, groups = dim)\n",
    "        x_hl = torch.nn.functional.conv2d(x, w_hl.expand(dim, -1, -1, -1), stride = 2, groups = dim)\n",
    "        x_hh = torch.nn.functional.conv2d(x, w_hh.expand(dim, -1, -1, -1), stride = 2, groups = dim)\n",
    "        x = torch.cat([x_ll, x_lh, x_hl, x_hh], dim=1)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dx):\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            w_ll, w_lh, w_hl, w_hh = ctx.saved_tensors\n",
    "            B, C, H, W = ctx.shape\n",
    "            dx = dx.view(B, 4, -1, H//2, W//2)\n",
    "\n",
    "            dx = dx.transpose(1,2).reshape(B, -1, H//2, W//2)\n",
    "            filters = torch.cat([w_ll, w_lh, w_hl, w_hh], dim=0).repeat(C, 1, 1, 1)\n",
    "            dx = torch.nn.functional.conv_transpose2d(dx, filters, stride=2, groups=C)\n",
    "\n",
    "        return dx, None, None, None, None\n",
    "\n",
    "class IDWT_Function(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, filters):\n",
    "        ctx.save_for_backward(filters)\n",
    "        ctx.shape = x.shape\n",
    "\n",
    "        B, _, H, W = x.shape\n",
    "        x = x.view(B, 4, -1, H, W).transpose(1, 2)\n",
    "        C = x.shape[1]\n",
    "        x = x.reshape(B, -1, H, W)\n",
    "        filters = filters.repeat(C, 1, 1, 1)\n",
    "        x = torch.nn.functional.conv_transpose2d(x, filters, stride=2, groups=C)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dx):\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            filters = ctx.saved_tensors\n",
    "            filters = filters[0]\n",
    "            B, C, H, W = ctx.shape\n",
    "            C = C // 4\n",
    "            dx = dx.contiguous()\n",
    "\n",
    "            w_ll, w_lh, w_hl, w_hh = torch.unbind(filters, dim=0)\n",
    "            x_ll = torch.nn.functional.conv2d(dx, w_ll.unsqueeze(1).expand(C, -1, -1, -1), stride = 2, groups = C)\n",
    "            x_lh = torch.nn.functional.conv2d(dx, w_lh.unsqueeze(1).expand(C, -1, -1, -1), stride = 2, groups = C)\n",
    "            x_hl = torch.nn.functional.conv2d(dx, w_hl.unsqueeze(1).expand(C, -1, -1, -1), stride = 2, groups = C)\n",
    "            x_hh = torch.nn.functional.conv2d(dx, w_hh.unsqueeze(1).expand(C, -1, -1, -1), stride = 2, groups = C)\n",
    "            dx = torch.cat([x_ll, x_lh, x_hl, x_hh], dim=1)\n",
    "        return dx, None\n",
    "\n",
    "class IDWT_2D(nn.Module):\n",
    "    def __init__(self, wave):\n",
    "        super(IDWT_2D, self).__init__()\n",
    "        w = pywt.Wavelet(wave)\n",
    "        rec_hi = torch.Tensor(w.rec_hi)\n",
    "        rec_lo = torch.Tensor(w.rec_lo)\n",
    "        \n",
    "        w_ll = rec_lo.unsqueeze(0)*rec_lo.unsqueeze(1)\n",
    "        w_lh = rec_lo.unsqueeze(0)*rec_hi.unsqueeze(1)\n",
    "        w_hl = rec_hi.unsqueeze(0)*rec_lo.unsqueeze(1)\n",
    "        w_hh = rec_hi.unsqueeze(0)*rec_hi.unsqueeze(1)\n",
    "\n",
    "        w_ll = w_ll.unsqueeze(0).unsqueeze(1)\n",
    "        w_lh = w_lh.unsqueeze(0).unsqueeze(1)\n",
    "        w_hl = w_hl.unsqueeze(0).unsqueeze(1)\n",
    "        w_hh = w_hh.unsqueeze(0).unsqueeze(1)\n",
    "        filters = torch.cat([w_ll, w_lh, w_hl, w_hh], dim=0)\n",
    "        self.register_buffer('filters', filters)\n",
    "        self.filters = self.filters.to(dtype=torch.float32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return IDWT_Function.apply(x, self.filters)\n",
    "\n",
    "class DWT_2D(nn.Module):\n",
    "    def __init__(self, wave):\n",
    "        super(DWT_2D, self).__init__()\n",
    "        w = pywt.Wavelet(wave)\n",
    "        dec_hi = torch.Tensor(w.dec_hi[::-1]) \n",
    "        dec_lo = torch.Tensor(w.dec_lo[::-1])\n",
    "\n",
    "        w_ll = dec_lo.unsqueeze(0)*dec_lo.unsqueeze(1)\n",
    "        w_lh = dec_lo.unsqueeze(0)*dec_hi.unsqueeze(1)\n",
    "        w_hl = dec_hi.unsqueeze(0)*dec_lo.unsqueeze(1)\n",
    "        w_hh = dec_hi.unsqueeze(0)*dec_hi.unsqueeze(1)\n",
    "\n",
    "        self.register_buffer('w_ll', w_ll.unsqueeze(0).unsqueeze(0))\n",
    "        self.register_buffer('w_lh', w_lh.unsqueeze(0).unsqueeze(0))\n",
    "        self.register_buffer('w_hl', w_hl.unsqueeze(0).unsqueeze(0))\n",
    "        self.register_buffer('w_hh', w_hh.unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "        self.w_ll = self.w_ll.to(dtype=torch.float32)\n",
    "        self.w_lh = self.w_lh.to(dtype=torch.float32)\n",
    "        self.w_hl = self.w_hl.to(dtype=torch.float32)\n",
    "        self.w_hh = self.w_hh.to(dtype=torch.float32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return DWT_Function.apply(x, self.w_ll, self.w_lh, self.w_hl, self.w_hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwt = DWT_2D('haar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dwt = dwt(torch.rand([1, 256, 256, 30])).reshape(1, 256 // 2, 256 // 2,  30 * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 30 * 4\n",
    "qkv = nn.Linear(dim, dim * 3, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 120, 128, 128])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv(x_dwt).reshape(1, 256 // 2, 256 // 2, 3, 30 * 4).permute(3, 0, 4, 1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cbt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
